"""
==============================================================================
SCRIPT: An√°lisis Descriptivo - Terremotos USGS
==============================================================================
Autor: Jaime
Proyecto: Miner√≠a de Datos - An√°lisis de Terremotos Globales
Fecha: Noviembre 2025

DESCRIPCI√ìN:
Este script realiza el an√°lisis estad√≠stico descriptivo del dataset de 
terremotos ya limpio. Calcula m√©tricas clave y genera insights para el
informe t√©cnico.

AN√ÅLISIS REALIZADOS:
  1. Estad√≠sticas descriptivas de magnitud y profundidad
  2. Distribuci√≥n temporal (sismos por a√±o, d√©cada)
  3. An√°lisis geogr√°fico (regiones m√°s afectadas)
  4. Correlaciones entre variables
  5. Identificaci√≥n de eventos extremos

INPUT:
  - Archivo: data/processed/earthquakes_clean.csv

OUTPUT:
  - Reporte estad√≠stico: outputs/results/descriptive_statistics.txt
  - Tablas de frecuencia: outputs/results/frequency_tables.csv
==============================================================================
"""

import pandas as pd
import numpy as np
from scipy import stats
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ==============================================================================
# CONFIGURACI√ìN DE RUTAS
# ==============================================================================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
CLEAN_DATA_PATH = os.path.join(BASE_DIR, 'data', 'processed', 'earthquakes_clean.csv')
RESULTS_PATH = os.path.join(BASE_DIR, 'outputs', 'results')

print("="*80)
print("AN√ÅLISIS DESCRIPTIVO - TERREMOTOS USGS")
print("="*80)
print(f"Dataset: {CLEAN_DATA_PATH}")
print("="*80 + "\n")


# ==============================================================================
# FUNCI√ìN 1: CARGAR DATOS LIMPIOS
# ==============================================================================
def load_clean_data(file_path):
    """
    Carga el dataset limpio.
    
    Par√°metros:
    -----------
    file_path : str
        Ruta al archivo CSV limpio
    
    Retorna:
    --------
    pandas.DataFrame
        Dataset limpio cargado
    """
    print("[1/7] Cargando datos limpios...")
    
    try:
        df = pd.read_csv(file_path, parse_dates=['time'])
        print(f"   ‚úì Datos cargados: {len(df):,} registros")
        print(f"   ‚úì Columnas disponibles: {len(df.columns)}")
        print(f"   ‚úì Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n")
        return df
    
    except FileNotFoundError:
        print(f"‚ùå ERROR: No se encontr√≥ el archivo {file_path}")
        print("   Por favor, ejecuta primero el script 01_data_cleaning.py")
        return None
    except Exception as e:
        print(f"‚ùå ERROR al cargar datos: {str(e)}")
        return None


# ==============================================================================
# FUNCI√ìN 2: ESTAD√çSTICAS DESCRIPTIVAS DE MAGNITUD Y PROFUNDIDAD
# ==============================================================================
def calculate_basic_statistics(df):
    """
    Calcula estad√≠sticas descriptivas para magnitud y profundidad.
    
    NOTA PARA INFORME:
    "Se calcularon medidas de tendencia central y dispersi√≥n para las 
    variables magnitud y profundidad, permitiendo caracterizar la 
    distribuci√≥n de los eventos s√≠smicos."
    
    PARA PRESENTACI√ìN:
    "La magnitud promedio de los terremotos registrados es de X.X, con 
    una desviaci√≥n est√°ndar de Y.Y. La profundidad media es de Z.Z km."
    """
    print("[2/7] Calculando estad√≠sticas descriptivas b√°sicas...")
    
    stats_dict = {}
    
    # Estad√≠sticas para MAGNITUD
    if 'mag' in df.columns:
        print("\n   üìä MAGNITUD (mag):")
        mag_stats = {
            'count': df['mag'].count(),
            'mean': df['mag'].mean(),
            'median': df['mag'].median(),
            'std': df['mag'].std(),
            'min': df['mag'].min(),
            'q1': df['mag'].quantile(0.25),
            'q3': df['mag'].quantile(0.75),
            'max': df['mag'].max(),
            'iqr': df['mag'].quantile(0.75) - df['mag'].quantile(0.25),
            'cv': (df['mag'].std() / df['mag'].mean()) * 100  # Coeficiente de variaci√≥n
        }
        
        stats_dict['magnitud'] = mag_stats
        
        print(f"      Media (promedio):         {mag_stats['mean']:.3f}")
        print(f"      Mediana (Q2):             {mag_stats['median']:.3f}")
        print(f"      Desviaci√≥n est√°ndar:      {mag_stats['std']:.3f}")
        print(f"      M√≠nimo:                   {mag_stats['min']:.3f}")
        print(f"      Cuartil 1 (Q1):           {mag_stats['q1']:.3f}")
        print(f"      Cuartil 3 (Q3):           {mag_stats['q3']:.3f}")
        print(f"      M√°ximo:                   {mag_stats['max']:.3f}")
        print(f"      Rango intercuart√≠lico:    {mag_stats['iqr']:.3f}")
        print(f"      Coef. de variaci√≥n:       {mag_stats['cv']:.2f}%")
    
    # Estad√≠sticas para PROFUNDIDAD
    if 'depth' in df.columns:
        print("\n   üìä PROFUNDIDAD (depth en km):")
        depth_stats = {
            'count': df['depth'].count(),
            'mean': df['depth'].mean(),
            'median': df['depth'].median(),
            'std': df['depth'].std(),
            'min': df['depth'].min(),
            'q1': df['depth'].quantile(0.25),
            'q3': df['depth'].quantile(0.75),
            'max': df['depth'].max(),
            'iqr': df['depth'].quantile(0.75) - df['depth'].quantile(0.25),
            'cv': (df['depth'].std() / df['depth'].mean()) * 100
        }
        
        stats_dict['profundidad'] = depth_stats
        
        print(f"      Media (promedio):         {depth_stats['mean']:.2f} km")
        print(f"      Mediana (Q2):             {depth_stats['median']:.2f} km")
        print(f"      Desviaci√≥n est√°ndar:      {depth_stats['std']:.2f} km")
        print(f"      M√≠nimo:                   {depth_stats['min']:.2f} km")
        print(f"      Cuartil 1 (Q1):           {depth_stats['q1']:.2f} km")
        print(f"      Cuartil 3 (Q3):           {depth_stats['q3']:.2f} km")
        print(f"      M√°ximo:                   {depth_stats['max']:.2f} km")
        print(f"      Rango intercuart√≠lico:    {depth_stats['iqr']:.2f} km")
        print(f"      Coef. de variaci√≥n:       {depth_stats['cv']:.2f}%")
    
    print("\n   ‚úì Estad√≠sticas b√°sicas calculadas\n")
    return stats_dict


# ==============================================================================
# FUNCI√ìN 3: DISTRIBUCI√ìN TEMPORAL (SISMOS POR A√ëO Y D√âCADA)
# ==============================================================================
def analyze_temporal_distribution(df):
    """
    Analiza la distribuci√≥n temporal de los terremotos.
    
    NOTA PARA INFORME:
    "Se analiz√≥ la distribuci√≥n temporal de eventos s√≠smicos, revelando 
    patrones de frecuencia a lo largo de d√©cadas y a√±os espec√≠ficos."
    
    PARA PRESENTACI√ìN:
    "Se observa un incremento significativo en el registro de sismos a 
    partir de la d√©cada de 1960, posiblemente debido a la expansi√≥n de 
    la red sismogr√°fica global."
    """
    print("[3/7] Analizando distribuci√≥n temporal...")
    
    temporal_stats = {}
    
    # Sismos por a√±o
    if 'year' in df.columns:
        yearly_counts = df['year'].value_counts().sort_index()
        temporal_stats['por_anio'] = yearly_counts
        
        print("\n   üìÖ DISTRIBUCI√ìN POR A√ëO:")
        print(f"      A√±os analizados:          {int(df['year'].min())} - {int(df['year'].max())}")
        print(f"      Total de a√±os:            {int(df['year'].max() - df['year'].min() + 1)}")
        print(f"      Promedio sismos/a√±o:      {yearly_counts.mean():.0f}")
        print(f"      Mediana sismos/a√±o:       {yearly_counts.median():.0f}")
        print(f"      A√±o con m√°s sismos:       {yearly_counts.idxmax()} ({yearly_counts.max():,} sismos)")
        print(f"      A√±o con menos sismos:     {yearly_counts.idxmin()} ({yearly_counts.min():,} sismos)")
    
    # Sismos por d√©cada
    if 'decade' in df.columns:
        decade_counts = df['decade'].value_counts().sort_index()
        temporal_stats['por_decada'] = decade_counts
        
        print("\n   üìÖ DISTRIBUCI√ìN POR D√âCADA:")
        for decade in sorted(decade_counts.index):
            count = decade_counts[decade]
            pct = (count / len(df)) * 100
            print(f"      {int(decade)}s:  {count:>12,} sismos ({pct:>5.2f}%)")
        
        # Identificar tendencias
        decades_list = sorted(decade_counts.index)
        if len(decades_list) >= 2:
            recent_decades = decades_list[-3:]  # √öltimas 3 d√©cadas
            old_decades = decades_list[:3]       # Primeras 3 d√©cadas
            
            recent_avg = decade_counts[recent_decades].mean()
            old_avg = decade_counts[old_decades].mean()
            
            if recent_avg > old_avg:
                increase_factor = recent_avg / old_avg
                print(f"\n      üìà Tendencia: Incremento de {increase_factor:.1f}x en registros")
                print(f"         (comparando primeras vs √∫ltimas 3 d√©cadas)")
    
    # Sismos por mes (estacionalidad)
    if 'month' in df.columns:
        monthly_counts = df['month'].value_counts().sort_index()
        temporal_stats['por_mes'] = monthly_counts
        
        months_names = ['Ene', 'Feb', 'Mar', 'Abr', 'May', 'Jun', 
                       'Jul', 'Ago', 'Sep', 'Oct', 'Nov', 'Dic']
        
        print("\n   üìÖ DISTRIBUCI√ìN POR MES (promedio):")
        for month in range(1, 13):
            if month in monthly_counts.index:
                count = monthly_counts[month]
                print(f"      {months_names[month-1]}:  {count:>12,} sismos")
    
    print("\n   ‚úì An√°lisis temporal completado\n")
    return temporal_stats


# ==============================================================================
# FUNCI√ìN 4: AN√ÅLISIS GEOGR√ÅFICO (REGIONES M√ÅS AFECTADAS)
# ==============================================================================
def analyze_geographic_distribution(df, top_n=20):
    """
    Analiza las regiones m√°s afectadas por terremotos.
    
    Par√°metros:
    -----------
    top_n : int
        N√∫mero de regiones principales a mostrar
    
    NOTA PARA INFORME:
    "Se identificaron las regiones geogr√°ficas con mayor actividad s√≠smica 
    mediante el an√°lisis de la columna 'place', extrayendo informaci√≥n de 
    ubicaci√≥n a nivel de pa√≠s/regi√≥n."
    
    PARA PRESENTACI√ìN:
    "Las regiones con mayor actividad s√≠smica incluyen: [lista de top 5], 
    concentrando el X% de los eventos registrados."
    """
    print(f"[4/7] Analizando distribuci√≥n geogr√°fica (Top {top_n})...")
    
    geographic_stats = {}
    
    if 'place' not in df.columns:
        print("   ‚ö† Columna 'place' no encontrada, saltando an√°lisis geogr√°fico\n")
        return geographic_stats
    
    # Extraer pa√≠s/regi√≥n de la columna 'place'
    # Formato t√≠pico: "13km ESE of Volcano, Hawaii" -> extraer "Hawaii"
    # O: "Japan" -> extraer "Japan"
    
    def extract_region(place_str):
        """Extrae la regi√≥n/pa√≠s de la descripci√≥n del lugar."""
        if pd.isna(place_str):
            return 'Unknown'
        
        # Muchos lugares tienen formato "X km DIR of Location, Country"
        if ',' in place_str:
            parts = place_str.split(',')
            return parts[-1].strip()  # √öltima parte suele ser el pa√≠s/regi√≥n
        else:
            # Si no hay coma, extraer palabras capitalizadas al final
            words = place_str.split()
            if len(words) > 0:
                return words[-1]
            return 'Unknown'
    
    print("   Extrayendo informaci√≥n de regiones...")
    df['region'] = df['place'].apply(extract_region)
    
    # Contar sismos por regi√≥n
    region_counts = df['region'].value_counts().head(top_n)
    geographic_stats['por_region'] = region_counts
    
    print(f"\n   üåç TOP {top_n} REGIONES CON M√ÅS SISMOS:")
    print("   " + "-" * 70)
    print(f"   {'#':<4} {'Regi√≥n':<35} {'Sismos':>12} {'Porcentaje':>10}")
    print("   " + "-" * 70)
    
    for idx, (region, count) in enumerate(region_counts.items(), 1):
        pct = (count / len(df)) * 100
        print(f"   {idx:<4} {region:<35} {count:>12,} {pct:>9.2f}%")
    
    # Estad√≠sticas adicionales
    total_top_regions = region_counts.sum()
    pct_top_regions = (total_top_regions / len(df)) * 100
    
    print("   " + "-" * 70)
    print(f"   Total en Top {top_n}:                        {total_top_regions:>12,} {pct_top_regions:>9.2f}%")
    print("   " + "-" * 70)
    
    print("\n   ‚úì An√°lisis geogr√°fico completado\n")
    return geographic_stats


# ==============================================================================
# FUNCI√ìN 5: CORRELACI√ìN ENTRE MAGNITUD Y PROFUNDIDAD
# ==============================================================================
def analyze_correlations(df):
    """
    Calcula la correlaci√≥n entre magnitud y profundidad.
    
    NOTA PARA INFORME:
    "Se calcul√≥ el coeficiente de correlaci√≥n de Pearson entre magnitud y 
    profundidad para evaluar si existe una relaci√≥n lineal entre estas 
    variables."
    
    PARA PRESENTACI√ìN:
    "Se observ√≥ una correlaci√≥n [d√©bil/moderada/fuerte] de r=X.XX entre 
    magnitud y profundidad, sugiriendo que [interpretaci√≥n]."
    """
    print("[5/7] Analizando correlaciones...")
    
    correlation_stats = {}
    
    if 'mag' in df.columns and 'depth' in df.columns:
        # Eliminar NaN para el c√°lculo
        df_corr = df[['mag', 'depth']].dropna()
        
        # Correlaci√≥n de Pearson
        pearson_corr, pearson_pval = stats.pearsonr(df_corr['mag'], df_corr['depth'])
        
        # Correlaci√≥n de Spearman (no param√©trica)
        spearman_corr, spearman_pval = stats.spearmanr(df_corr['mag'], df_corr['depth'])
        
        correlation_stats['pearson'] = {
            'correlation': pearson_corr,
            'p_value': pearson_pval
        }
        
        correlation_stats['spearman'] = {
            'correlation': spearman_corr,
            'p_value': spearman_pval
        }
        
        print("\n   üìä CORRELACI√ìN: MAGNITUD vs PROFUNDIDAD")
        print("   " + "-" * 60)
        print(f"      Coeficiente de Pearson:   {pearson_corr:>7.4f}")
        print(f"      P-valor:                  {pearson_pval:.2e}")
        print(f"      Coeficiente de Spearman:  {spearman_corr:>7.4f}")
        print(f"      P-valor:                  {spearman_pval:.2e}")
        
        # Interpretaci√≥n
        print("\n   üí° INTERPRETACI√ìN:")
        abs_corr = abs(pearson_corr)
        if abs_corr < 0.3:
            strength = "d√©bil"
        elif abs_corr < 0.7:
            strength = "moderada"
        else:
            strength = "fuerte"
        
        direction = "positiva" if pearson_corr > 0 else "negativa"
        
        print(f"      Existe una correlaci√≥n {strength} {direction} (r={pearson_corr:.4f})")
        
        if pearson_pval < 0.05:
            print(f"      La correlaci√≥n es estad√≠sticamente significativa (p<0.05)")
        else:
            print(f"      La correlaci√≥n NO es estad√≠sticamente significativa (p‚â•0.05)")
        
        # Frase sugerida para el informe
        print("\n   üìù FRASE SUGERIDA PARA INFORME:")
        print(f"      \"Se observa una correlaci√≥n {strength} {direction} entre magnitud")
        print(f"       y profundidad (r={pearson_corr:.3f}, p{'<' if pearson_pval < 0.001 else '='}0.001),")
        if abs_corr < 0.3:
            print(f"       indicando que la profundidad tiene poca relaci√≥n lineal con la")
            print(f"       magnitud del terremoto.\"")
        else:
            print(f"       sugiriendo que ambas variables est√°n relacionadas.\"")
    
    print("\n   ‚úì An√°lisis de correlaciones completado\n")
    return correlation_stats


# ==============================================================================
# FUNCI√ìN 6: IDENTIFICACI√ìN DE EVENTOS EXTREMOS
# ==============================================================================
def identify_extreme_events(df, top_n=10):
    """
    Identifica los terremotos m√°s significativos (mayor magnitud).
    
    NOTA PARA INFORME:
    "Se identificaron los eventos s√≠smicos de mayor magnitud registrados 
    en el dataset, incluyendo informaci√≥n de ubicaci√≥n, profundidad y fecha."
    
    PARA PRESENTACI√ìN:
    "El terremoto de mayor magnitud registrado fue de X.X en [ubicaci√≥n], 
    ocurrido el [fecha]."
    """
    print(f"[6/7] Identificando eventos extremos (Top {top_n})...")
    
    if 'mag' not in df.columns:
        print("   ‚ö† No se puede identificar eventos extremos sin columna 'mag'\n")
        return None
    
    # Ordenar por magnitud descendente
    top_events = df.nlargest(top_n, 'mag')
    
    print(f"\n   ‚ö†Ô∏è  TOP {top_n} TERREMOTOS DE MAYOR MAGNITUD:")
    print("   " + "=" * 78)
    
    for idx, row in enumerate(top_events.itertuples(), 1):
        print(f"\n   #{idx}")
        print(f"      Magnitud:       {row.mag:.2f}")
        
        if hasattr(row, 'place'):
            print(f"      Ubicaci√≥n:      {row.place}")
        
        if hasattr(row, 'time'):
            print(f"      Fecha:          {row.time}")
        
        if hasattr(row, 'depth'):
            print(f"      Profundidad:    {row.depth:.2f} km")
        
        if hasattr(row, 'latitude') and hasattr(row, 'longitude'):
            print(f"      Coordenadas:    {row.latitude:.4f}¬∞N, {row.longitude:.4f}¬∞E")
    
    print("\n   " + "=" * 78)
    
    # Estad√≠sticas de eventos de alta magnitud
    high_magnitude_threshold = 7.0
    high_mag_count = (df['mag'] >= high_magnitude_threshold).sum()
    high_mag_pct = (high_mag_count / len(df)) * 100
    
    print(f"\n   üìä EVENTOS DE ALTA MAGNITUD (‚â•{high_magnitude_threshold}):")
    print(f"      Total:                    {high_mag_count:,} eventos")
    print(f"      Porcentaje del total:     {high_mag_pct:.3f}%")
    
    print("\n   ‚úì Eventos extremos identificados\n")
    
    return top_events


# ==============================================================================
# FUNCI√ìN 7: GENERAR REPORTE COMPLETO
# ==============================================================================
def generate_comprehensive_report(df, stats_dict, temporal_stats, 
                                  geographic_stats, correlation_stats):
    """
    Genera un reporte de texto completo con todos los resultados.
    """
    print("[7/7] Generando reporte completo...")
    
    report_path = os.path.join(RESULTS_PATH, 'descriptive_statistics.txt')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("REPORTE DE AN√ÅLISIS DESCRIPTIVO - TERREMOTOS USGS\n")
        f.write("=" * 80 + "\n")
        f.write(f"Fecha de generaci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Total de registros analizados: {len(df):,}\n")
        f.write("=" * 80 + "\n\n")
        
        # 1. ESTAD√çSTICAS DE MAGNITUD
        f.write("1. ESTAD√çSTICAS DESCRIPTIVAS - MAGNITUD\n")
        f.write("-" * 80 + "\n")
        if 'magnitud' in stats_dict:
            mag_stats = stats_dict['magnitud']
            f.write(f"   Media:                    {mag_stats['mean']:>10.3f}\n")
            f.write(f"   Mediana:                  {mag_stats['median']:>10.3f}\n")
            f.write(f"   Desviaci√≥n est√°ndar:      {mag_stats['std']:>10.3f}\n")
            f.write(f"   M√≠nimo:                   {mag_stats['min']:>10.3f}\n")
            f.write(f"   Cuartil 1:                {mag_stats['q1']:>10.3f}\n")
            f.write(f"   Cuartil 3:                {mag_stats['q3']:>10.3f}\n")
            f.write(f"   M√°ximo:                   {mag_stats['max']:>10.3f}\n")
            f.write(f"   Rango intercuart√≠lico:    {mag_stats['iqr']:>10.3f}\n")
            f.write(f"   Coef. de variaci√≥n:       {mag_stats['cv']:>10.2f}%\n")
        f.write("\n")
        
        # 2. ESTAD√çSTICAS DE PROFUNDIDAD
        f.write("2. ESTAD√çSTICAS DESCRIPTIVAS - PROFUNDIDAD (km)\n")
        f.write("-" * 80 + "\n")
        if 'profundidad' in stats_dict:
            depth_stats = stats_dict['profundidad']
            f.write(f"   Media:                    {depth_stats['mean']:>10.2f} km\n")
            f.write(f"   Mediana:                  {depth_stats['median']:>10.2f} km\n")
            f.write(f"   Desviaci√≥n est√°ndar:      {depth_stats['std']:>10.2f} km\n")
            f.write(f"   M√≠nimo:                   {depth_stats['min']:>10.2f} km\n")
            f.write(f"   Cuartil 1:                {depth_stats['q1']:>10.2f} km\n")
            f.write(f"   Cuartil 3:                {depth_stats['q3']:>10.2f} km\n")
            f.write(f"   M√°ximo:                   {depth_stats['max']:>10.2f} km\n")
            f.write(f"   Rango intercuart√≠lico:    {depth_stats['iqr']:>10.2f} km\n")
            f.write(f"   Coef. de variaci√≥n:       {depth_stats['cv']:>10.2f}%\n")
        f.write("\n")
        
        # 3. DISTRIBUCI√ìN TEMPORAL POR D√âCADA
        f.write("3. DISTRIBUCI√ìN TEMPORAL - POR D√âCADA\n")
        f.write("-" * 80 + "\n")
        if 'por_decada' in temporal_stats:
            for decade in sorted(temporal_stats['por_decada'].index):
                count = temporal_stats['por_decada'][decade]
                pct = (count / len(df)) * 100
                f.write(f"   {int(decade)}s:  {count:>15,} sismos ({pct:>6.2f}%)\n")
        f.write("\n")
        
        # 4. REGIONES M√ÅS AFECTADAS
        f.write("4. REGIONES GEOGR√ÅFICAS M√ÅS AFECTADAS (Top 20)\n")
        f.write("-" * 80 + "\n")
        if 'por_region' in geographic_stats:
            for idx, (region, count) in enumerate(geographic_stats['por_region'].items(), 1):
                pct = (count / len(df)) * 100
                f.write(f"   {idx:>2}. {region:<40} {count:>10,} ({pct:>5.2f}%)\n")
        f.write("\n")
        
        # 5. CORRELACI√ìN
        f.write("5. CORRELACI√ìN: MAGNITUD vs PROFUNDIDAD\n")
        f.write("-" * 80 + "\n")
        if 'pearson' in correlation_stats:
            f.write(f"   Coeficiente de Pearson:   {correlation_stats['pearson']['correlation']:>10.4f}\n")
            f.write(f"   P-valor:                  {correlation_stats['pearson']['p_value']:>10.2e}\n")
            f.write(f"   Coeficiente de Spearman:  {correlation_stats['spearman']['correlation']:>10.4f}\n")
            f.write(f"   P-valor:                  {correlation_stats['spearman']['p_value']:>10.2e}\n")
        f.write("\n")
        
        f.write("=" * 80 + "\n")
        f.write("FIN DEL REPORTE\n")
        f.write("=" * 80 + "\n")
    
    print(f"   ‚úì Reporte guardado en: {report_path}")
    
    # Tambi√©n guardar tablas en CSV
    csv_path = os.path.join(RESULTS_PATH, 'frequency_tables.csv')
    
    if 'por_decada' in temporal_stats and 'por_region' in geographic_stats:
        # Crear DataFrame combinado
        decades_df = temporal_stats['por_decada'].reset_index()
        decades_df.columns = ['D√©cada', 'Frecuencia']
        decades_df.to_csv(csv_path.replace('.csv', '_decades.csv'), index=False)
        
        regions_df = geographic_stats['por_region'].reset_index()
        regions_df.columns = ['Regi√≥n', 'Frecuencia']
        regions_df.to_csv(csv_path.replace('.csv', '_regions.csv'), index=False)
        
        print(f"   ‚úì Tablas de frecuencia guardadas en: {RESULTS_PATH}")
    
    print()


# ==============================================================================
# PROGRAMA PRINCIPAL
# ==============================================================================
def main():
    """
    Funci√≥n principal que ejecuta todo el pipeline de an√°lisis descriptivo.
    """
    
    # Paso 1: Cargar datos limpios
    df = load_clean_data(CLEAN_DATA_PATH)
    
    if df is None:
        print("‚ùå No se pudo continuar sin datos. Abortando.")
        return
    
    # Paso 2: Estad√≠sticas b√°sicas
    stats_dict = calculate_basic_statistics(df)
    
    # Paso 3: Distribuci√≥n temporal
    temporal_stats = analyze_temporal_distribution(df)
    
    # Paso 4: Distribuci√≥n geogr√°fica
    geographic_stats = analyze_geographic_distribution(df, top_n=20)
    
    # Paso 5: Correlaciones
    correlation_stats = analyze_correlations(df)
    
    # Paso 6: Eventos extremos
    top_events = identify_extreme_events(df, top_n=10)
    
    # Paso 7: Generar reporte completo
    generate_comprehensive_report(df, stats_dict, temporal_stats, 
                                  geographic_stats, correlation_stats)
    
    print("=" * 80)
    print("‚úì‚úì‚úì AN√ÅLISIS DESCRIPTIVO COMPLETADO EXITOSAMENTE ‚úì‚úì‚úì")
    print("=" * 80)
    print("\nPr√≥ximos pasos:")
    print("  1. Revisar reportes en: outputs/results/")
    print("  2. Ejecutar visualizaciones: python scripts/python/03_visualizations.py")
    print("  3. Crear mapas en R: Rscript scripts/R/02_maps_visualization.R")
    print("=" * 80 + "\n")


# ==============================================================================
# EJECUCI√ìN DEL SCRIPT
# ==============================================================================
if __name__ == "__main__":
    main()
